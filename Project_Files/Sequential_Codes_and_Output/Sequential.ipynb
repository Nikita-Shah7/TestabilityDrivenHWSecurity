{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SCOAP VALUES From .txt**"
      ],
      "metadata": {
        "id": "3PIvgjF1f-7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chardet\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to detect file encoding\n",
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        raw_data = f.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    return result['encoding']\n",
        "\n",
        "# Function to extract data table from text using regex\n",
        "def extract_data_table(file_path, encoding):\n",
        "    with open(file_path, 'r', encoding=encoding) as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Updated regex pattern to match the data table, including the new patterns\n",
        "    table_pattern = r'(?:^\\S+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+$)|' \\\n",
        "                    r'(?:^\\\\[\\w\\/-]+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+$)|' \\\n",
        "                    r'(?:^\\s*dff.*$)|' \\\n",
        "                    r'(?:^\\s*G\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+$)|' \\\n",
        "                    r'(?:^\\s*n\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+$)'\n",
        "\n",
        "    # Find all matches\n",
        "    table_data = re.findall(table_pattern, content, re.MULTILINE)\n",
        "\n",
        "    # Split each matched line and create a list of lists\n",
        "    data_table = [re.split(r'\\s+', row.strip()) for row in table_data]\n",
        "\n",
        "    return data_table\n",
        "\n",
        "# Directory containing the text files\n",
        "folder_path = '/content/s35932'  # Adjust this path if necessary\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dataframes = []\n",
        "file_list = sorted([f for f in os.listdir(folder_path) if f.endswith('.txt')])\n",
        "\n",
        "# Process each file in the folder, sorted by filename\n",
        "for filename in file_list:\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    # Detect file encoding\n",
        "    file_encoding = detect_encoding(file_path)\n",
        "    print(f'Processing file: {filename}, Detected encoding: {file_encoding}')\n",
        "\n",
        "    try:\n",
        "        # Extract data table from the text using regex\n",
        "        data_table = extract_data_table(file_path, file_encoding)\n",
        "\n",
        "        if data_table:  # Check if data_table is not empty\n",
        "            # Create DataFrame from the extracted data table\n",
        "            data = pd.DataFrame(data_table, columns=[\"Signal\", \"CC0\", \"CC1\", \"SC0\", \"SC1\"])\n",
        "\n",
        "            # Add a suffix to each column name to identify the file\n",
        "            data.columns = [f\"{col}_{filename}\" if col != \"Signal\" else col for col in data.columns]\n",
        "\n",
        "            # Append the DataFrame to the list\n",
        "            dataframes.append(data)\n",
        "        else:\n",
        "            print(f\"No data found in {filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {filename}: {e}\")\n",
        "\n",
        "def highlight_changes(row, col, filename):\n",
        "    reference_col = f\"{col}_{file_list[0]}\"\n",
        "    if pd.notna(row[reference_col]) and pd.notna(row[f\"{col}_{filename}\"]) and row[f\"{col}_{filename}\"] != row[reference_col]:\n",
        "        return f\"{row[f'{col}_{filename}']} (changed)\"\n",
        "    return row[f\"{col}_{filename}\"]\n",
        "\n",
        "# Merge all DataFrames on 'Signal' column\n",
        "if dataframes:\n",
        "    combined_data = dataframes[0]\n",
        "    for df in dataframes[1:]:\n",
        "        combined_data = pd.merge(combined_data, df, on=\"Signal\", how=\"outer\")\n",
        "\n",
        "    # Apply conditional formatting to highlight changes\n",
        "    for filename in file_list[1:]:\n",
        "        for col in ['CC0', 'CC1', 'SC0', 'SC1']:\n",
        "            combined_data[f\"{col}_{filename}\"] = combined_data.apply(\n",
        "                lambda row: highlight_changes(row, col, filename), axis=1\n",
        "            )\n",
        "\n",
        "    # Display the combined DataFrame to verify the content\n",
        "    print(combined_data)\n",
        "\n",
        "    # Save the combined DataFrame to a CSV file\n",
        "    output_file = '/content/s35932/output.csv'  # Adjust this path if necessary\n",
        "    combined_data.to_csv(output_file, index=False)\n",
        "    print(f'All data successfully saved to {output_file}')\n",
        "else:\n",
        "    print(\"No data files processed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_0wmSkKtcBR",
        "outputId": "82b078e4-a8cd-4312-c8d5-be3923aaa1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: s35932.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T000.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T001.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T002.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T003.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T004.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T005.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T006.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T007.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T008.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T009.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T010.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T011.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T012.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T013.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T014.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T015.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T016.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T017.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T018.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T019.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T200.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T201.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T202.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T203.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T204.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T205.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T206.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T207.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T208.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T209.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T210.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T211.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T212.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T213.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T214.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T215.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T216.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T217.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T218.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T219.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T400.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T401.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T402.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T403.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T404.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T405.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T406.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T407.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T408.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T409.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T410.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T411.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T412.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T413.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T414.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T415.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T416.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T417.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T418.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T419.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T420.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T421.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T422.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T423.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T424.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T425.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T426.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T427.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T428.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T429.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T430.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T431.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T432.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T433.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T434.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T435.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T436.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T437.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T438.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T439.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T440.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T441.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T442.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T600.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T601.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T602.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T603.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T604.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T605.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T606.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T607.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T608.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T609.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T610.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T611.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T612.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T613.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T614.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T615.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T616.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T617.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T618.txt, Detected encoding: ascii\n",
            "Processing file: s35932_T619.txt, Detected encoding: ascii\n",
            "               Signal CC0_s35932.txt CC1_s35932.txt SC0_s35932.txt  \\\n",
            "0           DATA_0_22              1              1              0   \n",
            "1                 TM1              1              1              0   \n",
            "2            DATA_0_1              1              1              0   \n",
            "3            DATA_0_5              1              1              0   \n",
            "4           DATA_0_26              1              1              0   \n",
            "...               ...            ...            ...            ...   \n",
            "17327       tempn3686            NaN            NaN            NaN   \n",
            "17328      troj19_1n4            NaN            NaN            NaN   \n",
            "17329  Trigger_en1_19            NaN            NaN            NaN   \n",
            "17330      troj19_0n6            NaN            NaN            NaN   \n",
            "17331      troj19_0n7            NaN            NaN            NaN   \n",
            "\n",
            "      SC1_s35932.txt CC0_s35932_T000.txt CC1_s35932_T000.txt  \\\n",
            "0                  0                   1                   1   \n",
            "1                  0                   1                   1   \n",
            "2                  0                   1                   1   \n",
            "3                  0                   1                   1   \n",
            "4                  0                   1                   1   \n",
            "...              ...                 ...                 ...   \n",
            "17327            NaN                 NaN                 NaN   \n",
            "17328            NaN                 NaN                 NaN   \n",
            "17329            NaN                 NaN                 NaN   \n",
            "17330            NaN                 NaN                 NaN   \n",
            "17331            NaN                 NaN                 NaN   \n",
            "\n",
            "      SC0_s35932_T000.txt SC1_s35932_T000.txt CC0_s35932_T001.txt  ...  \\\n",
            "0                       0                   0                   1  ...   \n",
            "1                       0                   0                   1  ...   \n",
            "2                       0                   0                   1  ...   \n",
            "3                       0                   0                   1  ...   \n",
            "4                       0                   0                   1  ...   \n",
            "...                   ...                 ...                 ...  ...   \n",
            "17327                 NaN                 NaN                 NaN  ...   \n",
            "17328                 NaN                 NaN                 NaN  ...   \n",
            "17329                 NaN                 NaN                 NaN  ...   \n",
            "17330                 NaN                 NaN                 NaN  ...   \n",
            "17331                 NaN                 NaN                 NaN  ...   \n",
            "\n",
            "      SC0_s35932_T617.txt SC1_s35932_T617.txt CC0_s35932_T618.txt  \\\n",
            "0                       0                   0                   1   \n",
            "1                       0                   0                   1   \n",
            "2                       0                   0                   1   \n",
            "3                       0                   0                   1   \n",
            "4                       0                   0                   1   \n",
            "...                   ...                 ...                 ...   \n",
            "17327                 NaN                 NaN                 NaN   \n",
            "17328                 NaN                 NaN                 NaN   \n",
            "17329                 NaN                 NaN                 NaN   \n",
            "17330                 NaN                 NaN                 NaN   \n",
            "17331                 NaN                 NaN                 NaN   \n",
            "\n",
            "      CC1_s35932_T618.txt SC0_s35932_T618.txt SC1_s35932_T618.txt  \\\n",
            "0                       1                   0                   0   \n",
            "1                       1                   0                   0   \n",
            "2                       1                   0                   0   \n",
            "3                       1                   0                   0   \n",
            "4                       1                   0                   0   \n",
            "...                   ...                 ...                 ...   \n",
            "17327                 NaN                 NaN                 NaN   \n",
            "17328                 NaN                 NaN                 NaN   \n",
            "17329                 NaN                 NaN                 NaN   \n",
            "17330                 NaN                 NaN                 NaN   \n",
            "17331                 NaN                 NaN                 NaN   \n",
            "\n",
            "      CC0_s35932_T619.txt CC1_s35932_T619.txt SC0_s35932_T619.txt  \\\n",
            "0                       1                   1                   0   \n",
            "1                       1                   1                   0   \n",
            "2                       1                   1                   0   \n",
            "3                       1                   1                   0   \n",
            "4                       1                   1                   0   \n",
            "...                   ...                 ...                 ...   \n",
            "17327                  43                   5                   4   \n",
            "17328                  14                  36                   1   \n",
            "17329                   6                 137                   0   \n",
            "17330                   7                 315                   0   \n",
            "17331            99999998                 321            99999998   \n",
            "\n",
            "      SC1_s35932_T619.txt  \n",
            "0                       0  \n",
            "1                       0  \n",
            "2                       0  \n",
            "3                       0  \n",
            "4                       0  \n",
            "...                   ...  \n",
            "17327                   0  \n",
            "17328                   3  \n",
            "17329                  11  \n",
            "17330                  24  \n",
            "17331                  24  \n",
            "\n",
            "[17332 rows x 417 columns]\n",
            "All data successfully saved to /content/s35932/output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtering of nodes**"
      ],
      "metadata": {
        "id": "Qb26Iksyga0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import chardet\n",
        "from xlsxwriter import Workbook  # Make sure xlsxwriter is installed\n",
        "\n",
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        result = chardet.detect(file.read())\n",
        "    return result['encoding']\n",
        "\n",
        "def extract_nodes_from_text(text):\n",
        "    effect_nodes = []\n",
        "    activation_nodes = []\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    effect_section = False\n",
        "    activation_section = False\n",
        "\n",
        "    for line in lines:\n",
        "        if 'Effect:' in line:\n",
        "            effect_section = True\n",
        "            activation_section = False\n",
        "            continue\n",
        "        if 'Activation Condition:' in line:\n",
        "            effect_section = False\n",
        "            activation_section = True\n",
        "            continue\n",
        "        if 'TROJAN BODY:' in line:\n",
        "            effect_section = False\n",
        "            activation_section = False\n",
        "            continue\n",
        "\n",
        "        if effect_section:\n",
        "            if line.strip().startswith(('n', 'N', 'g', 'G')):\n",
        "                node = line.strip().split()[0]\n",
        "                effect_nodes.append(node)\n",
        "\n",
        "        if activation_section:\n",
        "            if line.strip().startswith(('n', 'N', 'g', 'G')):\n",
        "                nodes = line.strip().split()\n",
        "                activation_nodes.extend(nodes)\n",
        "\n",
        "    return effect_nodes, activation_nodes\n",
        "\n",
        "def extract_values_from_csv(input_csv, nodes, filename):\n",
        "    df = pd.read_csv(input_csv)\n",
        "    # Define possible column name patterns based on filename\n",
        "    columns_to_extract = [f'CC0_{filename}', f'CC1_{filename}', f'SC0_{filename}', f'SC1_{filename}']\n",
        "\n",
        "    # Filter data based on nodes and selected columns\n",
        "    selected_columns = [col for col in columns_to_extract if col in df.columns]\n",
        "    df = df[['Signal'] + selected_columns]\n",
        "\n",
        "    df.reset_index()\n",
        "\n",
        "    # Set 'Signal' column as the index\n",
        "    df.set_index('Signal', inplace=True)\n",
        "\n",
        "    # Filter data based on nodes\n",
        "    extracted_data = df[df.index.isin(nodes)]\n",
        "    return extracted_data.reset_index()  # Reset index to keep 'Signal' as a column\n",
        "\n",
        "def main():\n",
        "    log_folder = '/content/s35392'\n",
        "    output_excel_file = '/content/s35392/s35392.xlsx'\n",
        "    csv_file = '/content/s35932_SCOAP_VALUES.csv'\n",
        "\n",
        "    # Create an ExcelWriter object\n",
        "    with pd.ExcelWriter(output_excel_file, engine='xlsxwriter') as writer:\n",
        "        for filename in sorted(os.listdir(log_folder)):\n",
        "            if filename.endswith('.txt'):\n",
        "                input_text_file = os.path.join(log_folder, filename)\n",
        "\n",
        "                # Detect the encoding of the input text file\n",
        "                encoding = detect_encoding(input_text_file)\n",
        "                print(f\"Processing {filename} with encoding: {encoding}\")\n",
        "\n",
        "                # Read the input text file with the detected encoding\n",
        "                with open(input_text_file, 'r', encoding=encoding) as file:\n",
        "                    text = file.read()\n",
        "\n",
        "                # Extract node values from the text\n",
        "                effect_nodes, activation_nodes = extract_nodes_from_text(text)\n",
        "                print(f\"Effect Nodes: {effect_nodes}\")\n",
        "                print(f\"Activation Nodes: {activation_nodes}\")\n",
        "\n",
        "                # Read the input CSV file and extract the relevant data\n",
        "                effect_data = extract_values_from_csv(csv_file, effect_nodes, filename)\n",
        "                activation_data = extract_values_from_csv(csv_file, activation_nodes, filename)\n",
        "\n",
        "                # Combine both sets of data into a single DataFrame\n",
        "                combined_data = pd.concat([effect_data, activation_data], ignore_index=True)\n",
        "\n",
        "                # Create a new sheet for the log file in the Excel workbook\n",
        "                sheet_name = os.path.splitext(filename)[0]  # Use full filename as sheet name\n",
        "                combined_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "                print(f\"Data from {filename} has been processed and saved to {sheet_name} sheet.\")\n",
        "\n",
        "    print(f\"All data has been saved to {output_excel_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "bsPcrWi0gVMv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}